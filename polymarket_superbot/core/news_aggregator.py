"""
News Aggregator - Multi-source news scraping and ranking
Designed for OpenClaw web_search and web_fetch integration
"""

import time
from typing import List, Dict, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone


@dataclass
class NewsArticle:
    """Represents a news article"""
    title: str
    url: str
    source: str
    snippet: str
    published: datetime
    relevance_score: float = 0.0
    
    @property
    def age_hours(self) -> float:
        """Hours since publication"""
        delta = datetime.now(timezone.utc) - self.published
        return delta.total_seconds() / 3600


class NewsAggregator:
    """
    Aggregate news from multiple sources
    
    NOTE: This module is designed to work with OpenClaw's tools:
    - Use web_search for discovery
    - Use web_fetch for article content
    - Use LLM for relevance ranking
    
    For production use, integrate with your OpenClaw instance.
    """
    
    def __init__(self):
        self.cache = {}
        self.cache_ttl = 300  # Cache for 5 minutes
    
    def search_crypto_news(self, query: str, max_age_hours: int = 24) -> List[NewsArticle]:
        """
        Search for crypto news
        
        Args:
            query: Search query (e.g., "Bitcoin regulation")
            max_age_hours: Only include articles from last N hours
        
        Returns:
            List of NewsArticle objects
        
        NOTE: In production, use OpenClaw's web_search tool:
            web_search(query=query, count=10, freshness="pd")
        """
        
        # Mock implementation - replace with OpenClaw integration
        print(f"üîç Searching news for: {query}")
        
        # Example structure for OpenClaw integration:
        # results = web_search(query=query, count=10, freshness="pd")
        # articles = [self._parse_search_result(r) for r in results]
        
        # For now, return mock data
        return [
            NewsArticle(
                title=f"Mock article for: {query}",
                url="https://example.com",
                source="CoinDesk",
                snippet="This is a mock article. Replace with web_search results.",
                published=datetime.now(timezone.utc) - timedelta(hours=2),
                relevance_score=0.8
            )
        ]
    
    def aggregate_market_news(self, market_question: str, max_articles: int = 20) -> List[NewsArticle]:
        """
        Aggregate news for a specific market
        
        Args:
            market_question: The market question (e.g., "Will BTC hit $100K?")
            max_articles: Max articles to return
        
        Returns:
            Ranked list of relevant articles
        """
        
        # Extract keywords from question
        keywords = self._extract_keywords(market_question)
        
        # Search for each keyword
        all_articles = []
        for keyword in keywords[:3]:  # Top 3 keywords
            articles = self.search_crypto_news(keyword, max_age_hours=24)
            all_articles.extend(articles)
        
        # Deduplicate
        seen_urls = set()
        unique_articles = []
        for article in all_articles:
            if article.url not in seen_urls:
                seen_urls.add(article.url)
                unique_articles.append(article)
        
        # Rank by relevance (TODO: use LLM for scoring)
        ranked = sorted(unique_articles, key=lambda a: a.relevance_score, reverse=True)
        
        return ranked[:max_articles]
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Extract keywords from market question"""
        # Simple keyword extraction
        # TODO: Use LLM for better extraction
        
        important_words = []
        words = text.lower().split()
        
        # Filter stopwords
        stopwords = {"will", "the", "a", "an", "is", "are", "be", "by", "on", "in", "at"}
        
        for word in words:
            clean = word.strip("?!.,")
            if clean and clean not in stopwords and len(clean) > 3:
                important_words.append(clean)
        
        return important_words
    
    def fetch_article_content(self, url: str) -> str:
        """
        Fetch full article content
        
        NOTE: In production, use OpenClaw's web_fetch tool:
            web_fetch(url=url, extractMode="markdown")
        """
        
        # Mock implementation
        return f"Article content for: {url}"
    
    def get_twitter_sentiment(self, query: str) -> Dict:
        """
        Get Twitter sentiment for a topic
        
        NOTE: Use OpenClaw's bird skill for real implementation
        
        Returns:
            {"positive": float, "negative": float, "neutral": float}
        """
        
        # Mock sentiment
        return {
            "positive": 0.6,
            "negative": 0.2,
            "neutral": 0.2,
            "sample_tweets": []
        }
    
    def summarize_news(self, articles: List[NewsArticle]) -> str:
        """
        Create a summary of news articles
        
        NOTE: Use LLM for real summarization
        """
        
        if not articles:
            return "No recent news found."
        
        summary = f"Found {len(articles)} relevant articles:\n\n"
        
        for i, article in enumerate(articles[:5], 1):
            summary += f"{i}. [{article.source}] {article.title}\n"
            summary += f"   {article.snippet[:100]}...\n"
            summary += f"   ({article.age_hours:.1f}h ago)\n\n"
        
        return summary


# ============================================================================
# OPENCLAW INTEGRATION TEMPLATE
# ============================================================================

"""
PRODUCTION INTEGRATION WITH OPENCLAW:

from openclaw_tools import web_search, web_fetch

class NewsAggregatorOpenClaw(NewsAggregator):
    def __init__(self, openclaw_session):
        super().__init__()
        self.session = openclaw_session
    
    def search_crypto_news(self, query: str, max_age_hours: int = 24) -> List[NewsArticle]:
        # Use OpenClaw web_search
        freshness = "pd" if max_age_hours <= 24 else "pw"
        results = web_search(query=query, count=10, freshness=freshness)
        
        articles = []
        for r in results:
            article = NewsArticle(
                title=r.get("title", ""),
                url=r.get("url", ""),
                source=self._extract_domain(r["url"]),
                snippet=r.get("snippet", ""),
                published=self._parse_date(r.get("published", "")),
                relevance_score=0.5  # Will be scored by LLM
            )
            articles.append(article)
        
        return articles
    
    def fetch_article_content(self, url: str) -> str:
        # Use OpenClaw web_fetch
        content = web_fetch(url=url, extractMode="markdown")
        return content
"""


# ============================================================================
# CLI TESTING
# ============================================================================

if __name__ == "__main__":
    aggregator = NewsAggregator()
    
    print("\n" + "="*70)
    print("üì∞ NEWS AGGREGATOR TEST")
    print("="*70)
    
    # Test with a mock market
    market_question = "Will Bitcoin hit $100K by March 2026?"
    
    print(f"\nüîç Aggregating news for: {market_question}")
    articles = aggregator.aggregate_market_news(market_question, max_articles=10)
    
    print(f"\n‚úÖ Found {len(articles)} articles")
    
    # Summarize
    summary = aggregator.summarize_news(articles)
    print(f"\nüìã Summary:\n{summary}")
    
    print("\n" + "="*70)
    print("‚ö†Ô∏è  NOTE: This is a mock implementation.")
    print("For production, integrate with OpenClaw's web_search and web_fetch tools.")
    print("="*70)
